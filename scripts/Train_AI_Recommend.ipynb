{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f250f8a-5d61-494b-a8a7-8d65d4de5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install scikit-learn\n",
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e307e17-d5f4-4be7-bbb1-c82a74978cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "# !pip install underthesea scikit-learn requests pandas numpy beautifulsoup4 lxml # Uncomment this in Colab/Jupyter if needed\n",
    "from underthesea import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import json # ### FIX ###: Import json library\n",
    "# Thêm thư viện cho parallel crawl (nếu dùng)\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import traceback # For detailed error logging\n",
    "\n",
    "print(\"Block 1: Imports, Cấu hình Nâng cao và Hàm Tiện ích - Đang chạy...\")\n",
    "\n",
    "# --- 1. Cấu hình Nâng cao ---\n",
    "BASE_HOST = \"https://buudien.vn\"\n",
    "CSV_FILENAME = \"buudien_ocop_products_detailed_v2_rerun.csv\"\n",
    "BASE_URL_TEMPLATE = \"https://buudien.vn/home/Search/index.html?keyword=OCOP&page={page}\"\n",
    "\n",
    "# Trọng số Feature\n",
    "NAME_WEIGHT = 7\n",
    "\n",
    "# Tham số TF-IDF Tối ưu\n",
    "TFIDF_NGRAM_RANGE = (1, 2)\n",
    "TFIDF_MIN_DF = 2\n",
    "TFIDF_MAX_DF = 0.8\n",
    "TFIDF_MAX_FEATURES = 5000\n",
    "TFIDF_SUBLINEAR_TF = True\n",
    "TFIDF_NORM = 'l2'\n",
    "\n",
    "# Số lượng gợi ý\n",
    "TOP_N_RAW_RECS = 30 # Used for precomputing raw recommendations\n",
    "TOP_N_FINAL_RECS = 10 # Used for final display/testing\n",
    "\n",
    "# Tên file lưu trữ kết quả\n",
    "COSINE_SIM_MATRIX_FILE = 'cosine_similarity_matrix_v2_adv.npy'\n",
    "INDICES_MAP_FILE = 'product_indices_map_v2_adv.pkl'\n",
    "PRODUCT_MAP_JSON_FILE = 'product_id_name_map_v2_adv.json' # ### FIX ###: Use JSON extension explicitly\n",
    "PRECOMPUTED_RECS_JSON_FILE = 'precomputed_recommendations_v2_raw_adv.json' # ### FIX ###: Use JSON extension\n",
    "\n",
    "# Cấu hình crawl\n",
    "REQUEST_TIMEOUT = 30\n",
    "RETRY_ATTEMPTS = 3\n",
    "SLEEP_MIN = 0.5\n",
    "SLEEP_MAX = 1.5\n",
    "CRAWL_MAX_PAGES = 250 # Giới hạn để test nhanh hơn, tăng lên nếu cần crawl hết (e.g., 500 or more)\n",
    "CRAWL_EMPTY_STOP = 5\n",
    "MAX_CRAWL_WORKERS = 8\n",
    "\n",
    "# --- Cấu hình Tiền xử lý Nâng cao ---\n",
    "STOP_WORDS_FILE = 'vietnamese_stopwords.txt'\n",
    "# Tải file stop words nếu chưa có (ví dụ cho Colab)\n",
    "if not os.path.exists(STOP_WORDS_FILE):\n",
    "    print(f\"Đang tải file {STOP_WORDS_FILE}...\")\n",
    "    try:\n",
    "        stopwords_url = \"https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt\" # Example source\n",
    "        stopwords_resp = requests.get(stopwords_url)\n",
    "        stopwords_resp.raise_for_status()\n",
    "        with open(STOP_WORDS_FILE, 'w', encoding='utf-8') as f_sw:\n",
    "            f_sw.write(stopwords_resp.text)\n",
    "        print(\"Tải stop words thành công.\")\n",
    "    except Exception as download_err:\n",
    "        print(f\"Lỗi tải stop words: {download_err}. Sử dụng set rỗng.\")\n",
    "\n",
    "\n",
    "VIETNAMESE_STOP_WORDS = set()\n",
    "if os.path.exists(STOP_WORDS_FILE):\n",
    "    try:\n",
    "        with open(STOP_WORDS_FILE, 'r', encoding='utf-8') as f:\n",
    "            VIETNAMESE_STOP_WORDS = set(line.strip() for line in f if line.strip())\n",
    "        print(f\"Đã tải {len(VIETNAMESE_STOP_WORDS)} stop words.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải stop words từ file cục bộ: {e}. Sử dụng set rỗng.\")\n",
    "else:\n",
    "    print(f\"Không tìm thấy file stop words: {STOP_WORDS_FILE}. Sử dụng set rỗng.\")\n",
    "\n",
    "# Từ điển Đồng nghĩa (Giữ nguyên ví dụ của bạn)\n",
    "SYNONYM_DICT = {\n",
    "    # Tên Thương hiệu / Nhà sản xuất (Ví dụ)\n",
    "    'huongfarm': 'huongfarm', 'lạc lạc plus': 'laclacplus', 'phủ quỳ': 'phuquy',\n",
    "    'xuân anh': 'xuananh', 'đất ngọc': 'datngoc', 'gieo...đặc sản đà lạt': 'gieodacsandl',\n",
    "    'dương anh 568': 'duonganh568', 'viện nông nghiệp thanh hoá': 'viennnthanhhoa',\n",
    "    'hải đăng': 'haidang', 'bà ba': 'baba', 'quý thu': 'quythu', 'bà hùng': 'bahung',\n",
    "    'kim huệ': 'kimhue', 'hữu châu': 'huuchau', 'minh vạn': 'minhvan',\n",
    "    # ... (Thêm các từ đồng nghĩa khác nếu cần) ...\n",
    "    'ngũ cốc dinh dưỡng': 'ngucocdinhduong', 'ngũ cốc siêu dinh dưỡng': 'ngucocdinhduong',\n",
    "    'mì chùm ngây': 'michumngay', 'mì cà rốt': 'micarot', 'mì củ dền': 'micuden',\n",
    "    'bột sắn dây': 'botsanday', 'tinh bột sắn dây': 'botsanday', 'trà túi lọc': 'tratuiloc',\n",
    "    'trà xạ đen': 'traxaden', 'chè xanh': 'chexanh', 'trà sơn mật': 'trasonmat',\n",
    "    'hồng sâm': 'hongsam', 'trà hoa vàng': 'trahoavang', 'chè hảo đạt': 'chehaodat',\n",
    "    'tôm nõn': 'tomnon', 'trà hoàng thảo mộc': 'trahoangthaomoc', 'cà phê hòa tan': 'caphehoatan',\n",
    "    'mộng dừa': 'mongdua', 'đẳng sâm': 'dangsam', 'ngọc linh': 'ngoclinh', 'cà phê đăk hà': 'caphedakha',\n",
    "    'trà giải độc gan': 'tragiaidocgan', 'cà gai leo': 'cagaileo', 'xạ đen': 'xaden',\n",
    "    'tinh bột nghệ': 'tinhbotnghe', 'viên nghệ mật ong': 'viennghematong', 'nem chua': 'nemchua',\n",
    "    'nem nướng': 'nemnuong', 'mắm ruốc': 'mamruoc', 'mắm tôm': 'mamtom', 'mắm tép': 'mamtep',\n",
    "    'nước mắm': 'nuocmam', 'cá cơm': 'cacom', 'khô cá': 'khoca', 'cá lóc': 'caloc',\n",
    "    'cá sặc rằn': 'casacran', 'cá kèo': 'cakeo', 'cá bống': 'cabong', 'cá thu': 'cathu',\n",
    "    'cá mòi': 'camoi', 'cá nhệch': 'canhech', 'cá trắm': 'catram', 'bún gạo khô': 'bungaokho',\n",
    "    'bún khô': 'bunkho', 'hủ tiếu khô': 'hutieukho', 'miến gạo': 'miengao', 'miến dong': 'miendong',\n",
    "    'gạo lứt': 'gaolut', 'gạo tím than': 'gaotimthan', 'nếp cẩm': 'nepcam',\n",
    "    'đông trùng hạ thảo': 'dongtrunghathao', 'hạt mắc ca': 'macca', 'hạt macca': 'macca',\n",
    "    'macadamia': 'macca', 'hạt điều': 'hatdieu', 'rang muối': 'rangmuoi', 'bánh đa nem': 'banhdanem',\n",
    "    'bánh đa vừng': 'banhdavung', 'bánh tráng': 'banhtrang', 'cơm cháy': 'comchay',\n",
    "    'chà bông': 'chabong', 'sữa chua': 'suachua', 'yến sào': 'yensao', 'tổ yến': 'toyen',\n",
    "    'tinh dầu': 'tinhdau', 'sả chanh': 'sachanh', 'húng chanh': 'hungchanh', 'tía tô': 'tiato',\n",
    "    'sấy dẻo': 'saydeo', 'sấy khô': 'saykho', 'sấy giòn': 'saygion', 'sấy thăng hoa': 'saythanghoa',\n",
    "    'sấy truyền thống': 'saytruyenthong', 'ngâm muối': 'ngammuoi', 'muối chua': 'muoichua',\n",
    "    'hút chân không': 'hutchankhong', 'nguyên chất': 'nguyenchat', 'cao cấp': 'caocap',\n",
    "    'thượng hạng': 'thuonghang', 'hữu cơ': 'huuco', 'organic': 'huuco', 'túi lọc': 'tuiloc',\n",
    "    'dạng bột': 'dangbot', 'dạng viên': 'dangvien',\n",
    "    'lâm đồng': 'lamdong', 'đà lạt': 'dalat', 'bến tre': 'bentre', 'tiền giang': 'tiengiang',\n",
    "    'hà giang': 'hagiang', 'cao bằng': 'caobang', 'bắc kạn': 'backan', 'thái nguyên': 'thainguyen',\n",
    "    'ninh bình': 'ninhbinh', 'thanh hóa': 'thanhhoa', 'nghệ an': 'nghean', 'hà tĩnh': 'hatinh',\n",
    "    'quảng bình': 'quangbinh', 'quảng trị': 'quangtri', 'thừa thiên huế': 'hue',\n",
    "    'quảng nam': 'quangnam', 'quảng ngãi': 'quangngai', 'bình định': 'binhdinh', 'phú yên': 'phuyen',\n",
    "    'khánh hòa': 'khanhhoa', 'ninh thuận': 'ninhthuan', 'bình thuận': 'binhthuan', 'kon tum': 'kontum',\n",
    "    'gia lai': 'gialai', 'đắk lắk': 'daklak', 'đắc lắc': 'daklak', 'đắk nông': 'daknong',\n",
    "    'đắc nông': 'daknong', 'bình phước': 'binhphuoc', 'bình dương': 'binhduong', 'tây ninh': 'tayninh',\n",
    "    'đồng nai': 'dongnai', 'bà rịa vũng tàu': 'vungtau', 'long an': 'longan', 'đồng tháp': 'dongthap',\n",
    "    'an giang': 'angiang', 'cần thơ': 'cantho', 'vĩnh long': 'vinhlong', 'hậu giang': 'haugiang',\n",
    "    'sóc trăng': 'soctrang', 'bạc liêu': 'baclieu', 'cà mau': 'camau', 'kiên giang': 'kiengiang',\n",
    "    'phú quốc': 'phuquoc',\n",
    "}\n",
    "\n",
    "TOKEN_MIN_LEN = 2\n",
    "TOKEN_MAX_LEN = 20\n",
    "\n",
    "# --- 2. Hàm Tiện ích ---\n",
    "session = requests.Session()\n",
    "# Cập nhật User-Agent để tránh bị chặn\n",
    "session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'})\n",
    "\n",
    "def apply_synonyms(text):\n",
    "    \"\"\"Áp dụng thay thế từ đồng nghĩa.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    processed_text = text.lower() # Chuyển lowercase trước khi thay thế\n",
    "    for k, v in SYNONYM_DICT.items():\n",
    "        # Thay thế bằng regex để đảm bảo là từ riêng biệt (\\b)\n",
    "        processed_text = re.sub(r'\\b{}\\b'.format(re.escape(k)), v, processed_text, flags=re.IGNORECASE)\n",
    "    return processed_text\n",
    "\n",
    "def advanced_tokenizer(text):\n",
    "    \"\"\"Tokenizer nâng cao: synonyms, word_tokenize, stop words, length filter.\"\"\"\n",
    "    if not isinstance(text, str): return []\n",
    "\n",
    "    # 1. Áp dụng synonyms (đã chuyển lowercase trong apply_synonyms)\n",
    "    text = apply_synonyms(text)\n",
    "\n",
    "    # 2. Chuẩn hóa cơ bản (bỏ ký tự đặc biệt, số) - Giữ lại khoảng trắng và dấu gạch dưới (do underthesea)\n",
    "    text = re.sub(r'[^\\w\\s_]', '', text, flags=re.UNICODE) # Giữ gạch dưới cho từ ghép underthesea\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 3. Tokenize bằng underthesea\n",
    "    try:\n",
    "        # Đặt trong try-except vì underthesea có thể lỗi với input lạ\n",
    "        tokenized_text = word_tokenize(text, format=\"text\")\n",
    "        tokens = tokenized_text.split()\n",
    "    except Exception as tokenize_err:\n",
    "        # print(f\"Lỗi underthesea tokenizer cho text: '{text[:100]}...' - {tokenize_err}\")\n",
    "        tokens = text.split() # Fallback về split theo khoảng trắng\n",
    "\n",
    "    # 4. Lọc Stop words và giới hạn độ dài\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens\n",
    "        if token not in VIETNAMESE_STOP_WORDS and TOKEN_MIN_LEN <= len(token) <= TOKEN_MAX_LEN\n",
    "    ]\n",
    "\n",
    "    return filtered_tokens # Trả về list các token đã xử lý\n",
    "\n",
    "def safe_int_convert(value):\n",
    "    \"\"\"Chuyển đổi an toàn sang integer, trả về None nếu lỗi.\"\"\"\n",
    "    try:\n",
    "        # Xử lý trường hợp float trước khi chuyển int (ví dụ: 4.0)\n",
    "        return int(float(value))\n",
    "    except (ValueError, TypeError, OverflowError):\n",
    "        return None\n",
    "\n",
    "def clean_price(price_text):\n",
    "    \"\"\"Làm sạch và chuyển đổi giá sang integer.\"\"\"\n",
    "    if not isinstance(price_text, str): return 0\n",
    "    # Bỏ \"đ\", \".\", \",\", khoảng trắng và các ký tự không phải số\n",
    "    price_text = price_text.lower().replace('đ', '').replace('.', '').replace(',', '').strip()\n",
    "    price_cleaned = re.sub(r'[^\\d]', '', price_text)\n",
    "    price_int = safe_int_convert(price_cleaned)\n",
    "    return price_int if price_int is not None else 0 # Trả về 0 nếu không chuyển đổi được\n",
    "\n",
    "print(\"Block 1: Hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a872d51-c790-44d6-a9a4-8a3e1f9e4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Block 2: Định nghĩa các Hàm Crawl Dữ liệu (Phiên bản V2 - Fix Pagination) - Đang chạy...\")\n",
    "\n",
    "# --- Hàm Crawl Chi tiết Sản phẩm (Phiên bản V2 - Fix Lỗi 'match') ---\n",
    "def get_product_details_v2(product_url, existing_data):\n",
    "    \"\"\"Hàm crawl chi tiết sản phẩm, cố gắng lấy description, origin, producer, ocop_rating.\"\"\"\n",
    "    details = {\"description\": \"\", \"origin\": \"\", \"ocop_rating\": None, \"producer\": \"\"}\n",
    "    for attempt in range(RETRY_ATTEMPTS):\n",
    "        try:\n",
    "            # In log truy cập (có thể comment bớt nếu quá nhiều)\n",
    "            # print(f\"   - Đang truy cập chi tiết (v2): {product_url}\")\n",
    "            time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX)) # Giảm sleep khi dùng parallel\n",
    "            response = session.get(product_url, timeout=REQUEST_TIMEOUT)\n",
    "            response.raise_for_status() # Kiểm tra lỗi HTTP (4xx, 5xx)\n",
    "            soup = BeautifulSoup(response.text, 'lxml') # Sử dụng lxml cho parser mạnh mẽ hơn\n",
    "\n",
    "            # Lấy Description\n",
    "            desc_element = soup.select_one('div.wp_content_tab_description_product')\n",
    "            details[\"description\"] = ' '.join(desc_element.stripped_strings) if desc_element else \"\"\n",
    "\n",
    "            # Lấy Origin (Xuất xứ) - Ưu tiên từ thông tin giao hàng, sau đó đến bảng chi tiết\n",
    "            origin_text = \"\"\n",
    "            origin_gui_tu = soup.select_one('div.kv_store_info div.drop_kv_giaohang') # Selector có thể cần cập nhật\n",
    "            if origin_gui_tu and origin_gui_tu.text.strip():\n",
    "                 origin_text = ' '.join(origin_gui_tu.stripped_strings)\n",
    "\n",
    "            detail_table = soup.select_one('table.tb_parameter_product') # Selector cho bảng chi tiết\n",
    "            if detail_table and not origin_text: # Chỉ tìm trong bảng nếu chưa có từ nguồn khác\n",
    "                 for row in detail_table.find_all('tr'):\n",
    "                     cells = row.find_all('td')\n",
    "                     if len(cells) == 2:\n",
    "                         label = cells[0].text.strip().lower() # Chuyển label sang lowercase để so sánh dễ hơn\n",
    "                         value = cells[1].text.strip()\n",
    "                         if label and value and label in [\"xuất xứ\", \"nơi sản xuất\", \"tỉnh thành\"]: # So sánh lowercase\n",
    "                             origin_text = value\n",
    "                             break # Thoát khi tìm thấy\n",
    "            details[\"origin\"] = origin_text.strip()\n",
    "\n",
    "            # Lấy Producer (Nhà sản xuất/Thương hiệu/Gian hàng) - Ưu tiên tên shop, sau đó đến bảng\n",
    "            producer_text = \"\"\n",
    "            shop_name_element = soup.select_one('div.info_store h3 a') # Lấy link trong h3 nếu có\n",
    "            if shop_name_element and shop_name_element.text.strip():\n",
    "                producer_text = shop_name_element.text.strip()\n",
    "            elif shop_name_element is None: # Nếu không có link, thử lấy text của h3\n",
    "                 shop_name_element = soup.select_one('div.info_store h3')\n",
    "                 if shop_name_element and shop_name_element.text.strip():\n",
    "                     producer_text = shop_name_element.text.strip()\n",
    "\n",
    "            if detail_table and not producer_text: # Chỉ tìm trong bảng nếu chưa có\n",
    "                 for row in detail_table.find_all('tr'):\n",
    "                     cells = row.find_all('td')\n",
    "                     if len(cells) == 2:\n",
    "                         label = cells[0].text.strip().lower()\n",
    "                         value = cells[1].text.strip()\n",
    "                         if label and value and label in [\"thương hiệu\", \"nhà sản xuất\", \"gian hàng\"]:\n",
    "                             producer_text = value\n",
    "                             break\n",
    "            details[\"producer\"] = producer_text.strip()\n",
    "\n",
    "            # Lấy OCOP Rating - Ưu tiên từ list page, sau đó tìm trong description, cuối cùng là bảng\n",
    "            ocop_rating = existing_data.get(\"ocop_rating_from_list\") # Lấy từ list trước (đã là int hoặc None)\n",
    "            match = None # Initialize match to None\n",
    "\n",
    "            if ocop_rating is None: # Chỉ tìm nếu chưa có từ list\n",
    "                if details[\"description\"]:\n",
    "                    # Tìm trong description trước (regex tìm số sau 'OCOP'/'Hạng' và trước 'sao')\n",
    "                    match = re.search(r'(?:OCOP|Hạng)\\s*[:\\s-]*\\s*(\\d)\\s*sao', details[\"description\"], re.IGNORECASE)\n",
    "                    if match: # Kiểm tra ngay sau khi gán\n",
    "                        ocop_rating = safe_int_convert(match.group(1))\n",
    "\n",
    "                # Nếu vẫn chưa có, tìm trong bảng chi tiết\n",
    "                if detail_table and ocop_rating is None:\n",
    "                    for row in detail_table.find_all('tr'):\n",
    "                         cells = row.find_all('td')\n",
    "                         if len(cells) == 2:\n",
    "                             label = cells[0].text.strip().lower() # Lowercase label\n",
    "                             value = cells[1].text.strip()\n",
    "                             if label and value and label in [\"chứng nhận ocop\", \"hạng sao ocop\", \"ocop\"]:\n",
    "                                  # Tìm số đầu tiên trong value\n",
    "                                  match_table = re.search(r'(\\d)', value)\n",
    "                                  if match_table:\n",
    "                                      ocop_rating = safe_int_convert(match_table.group(1))\n",
    "                                      break # Thoát vòng lặp khi tìm thấy\n",
    "\n",
    "            details[\"ocop_rating\"] = ocop_rating # Gán kết quả cuối cùng (có thể vẫn là None)\n",
    "            return details # Trả về kết quả nếu thành công\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"   - Lỗi mạng/HTTP chi tiết (lần {attempt+1}/{RETRY_ATTEMPTS}) URL: {product_url} - Lỗi: {e}\")\n",
    "        except AttributeError as ae: # Lỗi thường gặp khi selector không tìm thấy element\n",
    "             print(f\"   - Lỗi AttributeError chi tiết (lần {attempt+1}/{RETRY_ATTEMPTS}) URL: {product_url} - Lỗi: {ae} (Kiểm tra selector)\")\n",
    "        except Exception as e:\n",
    "            print(f\"   - Lỗi khác khi crawl chi tiết (lần {attempt+1}/{RETRY_ATTEMPTS}) URL: {product_url} - Lỗi: {type(e).__name__} - {e}\")\n",
    "            # traceback.print_exc() # Uncomment để xem full traceback nếu cần debug sâu\n",
    "\n",
    "        if attempt == RETRY_ATTEMPTS - 1:\n",
    "            print(f\"   - !!! Bỏ qua chi tiết sau {RETRY_ATTEMPTS} lần thử: {product_url}\")\n",
    "            # Trả về dictionary với giá trị mặc định hoặc lỗi\n",
    "            details[\"description\"] = f\"Lỗi crawl chi tiết sau {RETRY_ATTEMPTS} lần thử.\"\n",
    "            return details\n",
    "        time.sleep(random.uniform(2, 4) * (attempt + 1)) # Backoff tăng dần giữa các lần thử\n",
    "\n",
    "    print(f\"   - !!! Lỗi không xác định sau vòng lặp retry cho URL: {product_url}\")\n",
    "    return details\n",
    "\n",
    "\n",
    "# --- Hàm Crawl Trang Danh sách (Phiên bản V2 - Fix Pagination) ---\n",
    "def crawl_product_list_page_v2(page_url):\n",
    "    \"\"\"Hàm crawl một trang danh sách sản phẩm, trả về list sản phẩm và tổng số trang (nếu tìm thấy).\"\"\"\n",
    "    products_on_page = []\n",
    "    total_pages_output = None # This will be the value returned by the function for total pages\n",
    "\n",
    "    try:\n",
    "        # print(f\"Đang crawl list page (v2): {page_url}\")\n",
    "        response = session.get(page_url, timeout=REQUEST_TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml') # Sử dụng lxml cho parser mạnh mẽ hơn\n",
    "\n",
    "        # --- Determine total_pages (robustly) ---\n",
    "        # This logic should ideally run before checking for product_boxes,\n",
    "        # as total_pages_output is useful even if the current page has no products (e.g. last page of results)\n",
    "        pagination_el = soup.select_one('ul.pagination')\n",
    "\n",
    "        if pagination_el:\n",
    "            # Priority 1: Extract from \"Trang X/Y\" text if available\n",
    "            page_info_element = pagination_el.select_one('li.totalPage span, li.totalPage em')\n",
    "            if page_info_element:\n",
    "                page_info_text = page_info_element.text.strip()\n",
    "                match_pages_info = re.search(r'Trang\\s*\\d+\\s*/\\s*(\\d+)', page_info_text, re.IGNORECASE)\n",
    "                if match_pages_info:\n",
    "                    total_pages_output = safe_int_convert(match_pages_info.group(1))\n",
    "                    # if total_pages_output:\n",
    "                    # print(f\"DEBUG P1 [Page {page_url.split('page=')[-1]}]: Total pages from 'Trang X/Y': {total_pages_output}\")\n",
    "\n",
    "\n",
    "            # Priority 2: If not found via P1, try the \"Cuối\" link\n",
    "            if total_pages_output is None:\n",
    "                cuoi_link = pagination_el.select_one('li:not(.disabled) a[href*=\"page=\"]:-soup-contains(\"Cuối\")')\n",
    "                if not cuoi_link:\n",
    "                     cuoi_link = pagination_el.select_one('li a[href*=\"page=\"]:-soup-contains(\"Cuối\")')\n",
    "\n",
    "                if cuoi_link:\n",
    "                    href_cuoi = cuoi_link.get('href')\n",
    "                    if href_cuoi:\n",
    "                        match_cuoi = re.search(r'page=(\\d+)', href_cuoi)\n",
    "                        if match_cuoi:\n",
    "                            total_pages_output = safe_int_convert(match_cuoi.group(1))\n",
    "                            # if total_pages_output:\n",
    "                            # print(f\"DEBUG P2 [Page {page_url.split('page=')[-1]}]: Total pages from 'Cuối' link: {total_pages_output}\")\n",
    "\n",
    "            # Priority 3: Fallback to the maximum page number found in any pagination link\n",
    "            if total_pages_output is None:\n",
    "                all_page_links = pagination_el.select('li a[href*=\"page=\"]')\n",
    "                candidate_page_numbers = []\n",
    "                if all_page_links:\n",
    "                    for link_el in all_page_links:\n",
    "                        href_link = link_el.get('href')\n",
    "                        if href_link:\n",
    "                            match_page_num_in_href = re.search(r'page=(\\d+)', href_link)\n",
    "                            if match_page_num_in_href:\n",
    "                                page_num = safe_int_convert(match_page_num_in_href.group(1))\n",
    "                                if page_num is not None:\n",
    "                                    candidate_page_numbers.append(page_num)\n",
    "                    if candidate_page_numbers:\n",
    "                        total_pages_output = max(candidate_page_numbers)\n",
    "                        # if total_pages_output:\n",
    "                        # print(f\"DEBUG P3 [Page {page_url.split('page=')[-1]}]: Total pages from max page num in links: {total_pages_output}\")\n",
    "        # else:\n",
    "            # print(f\"DEBUG [Page {page_url.split('page=')[-1]}]: No 'ul.pagination' element found.\")\n",
    "        # --- End Determine total_pages ---\n",
    "\n",
    "\n",
    "        product_boxes = soup.select('div.item_product_home')\n",
    "\n",
    "        if not product_boxes:\n",
    "            # print(f\"   - Trang {page_url} không có sản phẩm.\")\n",
    "            # Even if no products, total_pages_output might have been found.\n",
    "            # The main loop will handle stopping based on max_pages or empty page limits.\n",
    "            return products_on_page, total_pages_output # Return determined total_pages_output\n",
    "\n",
    "        # print(f\"   - Tìm thấy {len(product_boxes)} sản phẩm.\")\n",
    "        for i, box in enumerate(product_boxes):\n",
    "            product_info = {\"product_id\": None, \"name\": \"\", \"full_name\": \"\", \"price\": 0, \"image_url\": \"\", \"product_url\": \"\", \"short_description\": \"\", \"ocop_rating_from_list\": None}\n",
    "            link = box.select_one('div.img_product a')\n",
    "            href = link.get('href') if link else None\n",
    "            if href:\n",
    "                product_info[\"product_url\"] = urljoin(BASE_HOST, href)\n",
    "                match_id = re.search(r'goods_id=(\\d+)', href)\n",
    "                if match_id: product_info[\"product_id\"] = safe_int_convert(match_id.group(1))\n",
    "            else:\n",
    "                # print(f\"   - Box {i+1} thiếu link sản phẩm.\")\n",
    "                continue\n",
    "\n",
    "            content_link = box.select_one('div.content_product a')\n",
    "            if content_link:\n",
    "                product_info[\"full_name\"] = content_link.text.strip()\n",
    "                short_desc = content_link.get('alt') or content_link.get('title') or \"\"\n",
    "                product_info[\"short_description\"] = short_desc.strip()\n",
    "                match_rating = re.search(r'(\\d)\\s*sao\\s*(?:OCOP|$)|OCOP\\s*(\\d)\\s*sao', short_desc, re.IGNORECASE)\n",
    "                if match_rating:\n",
    "                    rating_str = match_rating.group(1) or match_rating.group(2)\n",
    "                    product_info[\"ocop_rating_from_list\"] = safe_int_convert(rating_str)\n",
    "\n",
    "            product_info[\"name\"] = re.sub(r'^✓?\\s*OCOP(?:\\s*\\d?\\s*sao)?\\s*[:\\s-]*', '', product_info[\"full_name\"], flags=re.IGNORECASE).strip()\n",
    "            if not product_info[\"name\"]: product_info[\"name\"] = product_info[\"full_name\"]\n",
    "\n",
    "            price_el = box.select_one('div.price_product')\n",
    "            product_info[\"price\"] = clean_price(price_el.text) if price_el else 0\n",
    "\n",
    "            img_el = box.select_one('div.img_product img')\n",
    "            img_src = img_el.get('data-src') or img_el.get('src') if img_el else None\n",
    "            if img_src: product_info[\"image_url\"] = urljoin(BASE_HOST, img_src)\n",
    "\n",
    "            if product_info[\"product_id\"] is not None:\n",
    "                products_on_page.append(product_info)\n",
    "            else:\n",
    "                print(f\"   - Cảnh báo: Không lấy được ID cho sản phẩm: {product_info['full_name']} tại URL: {product_info['product_url']}\")\n",
    "\n",
    "        return products_on_page, total_pages_output # Return products and determined total_pages_output\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"   - Lỗi mạng/HTTP khi crawl list page {page_url}: {e}\")\n",
    "        return [], None # Trả về list rỗng và None nếu lỗi mạng (total_pages_output will be None)\n",
    "    except Exception as e:\n",
    "        print(f\"   - Lỗi khác khi crawl list page {page_url}: {type(e).__name__} - {e}\")\n",
    "        # traceback.print_exc()\n",
    "        return [], None # Trả về list rỗng (total_pages_output will be None)\n",
    "\n",
    "\n",
    "# --- Hàm điều phối Crawl V2 (Parallel) ---\n",
    "def run_crawl_v2_parallel(start_page=1):\n",
    "    \"\"\"Hàm điều phối việc crawl, trước tiên lấy thông tin cơ bản tuần tự, sau đó crawl chi tiết song song.\"\"\"\n",
    "    print(\"\\n=== BẮT ĐẦU CRAWL DỮ LIỆU (V2 Logic - PARALLEL) ===\")\n",
    "    all_products_basic_info = [] # List chứa dict thông tin cơ bản\n",
    "    crawled_product_ids = set() # Set để tránh crawl trùng ID\n",
    "    current_page = start_page\n",
    "    max_pages = None # Sẽ được cập nhật khi crawl trang đầu tiên\n",
    "    consecutive_empty_pages = 0\n",
    "\n",
    "    print(\"\\n--- Giai đoạn 1: Thu thập thông tin cơ bản (tuần tự) ---\")\n",
    "    while True:\n",
    "        # Điều kiện dừng\n",
    "        if max_pages and current_page > max_pages:\n",
    "            print(f\"Đã crawl hết {max_pages} trang đã xác định.\")\n",
    "            break\n",
    "        if current_page > CRAWL_MAX_PAGES: # CRAWL_MAX_PAGES is an overall limit\n",
    "            print(f\"Đạt giới hạn {CRAWL_MAX_PAGES} trang crawl (CRAWL_MAX_PAGES).\")\n",
    "            break\n",
    "\n",
    "        page_url = BASE_URL_TEMPLATE.format(page=current_page)\n",
    "        products_on_page, total_pages_found_on_this_page = crawl_product_list_page_v2(page_url)\n",
    "\n",
    "        # Cập nhật tổng số trang nếu tìm thấy lần đầu VÀ nó hợp lệ ( > 0)\n",
    "        if total_pages_found_on_this_page is not None and total_pages_found_on_this_page > 0 and max_pages is None:\n",
    "             max_pages = total_pages_found_on_this_page\n",
    "             print(f\"==> Tổng số trang dự kiến được xác định: {max_pages}\")\n",
    "             # Adjust CRAWL_MAX_PAGES if it's set higher than the actual max_pages\n",
    "             # if CRAWL_MAX_PAGES > max_pages:\n",
    "             # CRAWL_MAX_PAGES = max_pages\n",
    "             # print(f\"    (Điều chỉnh CRAWL_MAX_PAGES thành {max_pages} để khớp với tổng trang tìm thấy)\")\n",
    "\n",
    "\n",
    "        # Xử lý trang trống\n",
    "        if not products_on_page:\n",
    "            # Kiểm tra lại xem có phải đã đến trang cuối dự kiến không\n",
    "            if max_pages and current_page >= max_pages:\n",
    "                print(f\"Trang {current_page} trống và là trang cuối dự kiến hoặc đã vượt qua. Dừng giai đoạn 1.\")\n",
    "                break\n",
    "            consecutive_empty_pages += 1\n",
    "            print(f\"Trang {current_page} trống (lần {consecutive_empty_pages}/{CRAWL_EMPTY_STOP}).\")\n",
    "            if consecutive_empty_pages >= CRAWL_EMPTY_STOP:\n",
    "                print(f\"Dừng giai đoạn 1 do {CRAWL_EMPTY_STOP} trang trống liên tiếp.\")\n",
    "                break\n",
    "        else:\n",
    "            # Reset bộ đếm trang trống và thêm sản phẩm mới\n",
    "            consecutive_empty_pages = 0\n",
    "            new_products_count = 0\n",
    "            for product in products_on_page:\n",
    "                pid = product.get('product_id')\n",
    "                # Chỉ thêm nếu có ID và ID chưa được crawl\n",
    "                if pid is not None and pid not in crawled_product_ids:\n",
    "                    all_products_basic_info.append(product)\n",
    "                    crawled_product_ids.add(pid)\n",
    "                    new_products_count += 1\n",
    "            if new_products_count > 0:\n",
    "                 print(f\"-> Trang {current_page}/{max_pages if max_pages else '?'}: Thêm {new_products_count} SP mới. Tổng cơ bản: {len(all_products_basic_info)}\")\n",
    "            else:\n",
    "                 print(f\"-> Trang {current_page}/{max_pages if max_pages else '?'}: Không có SP mới (có thể là trùng lặp ID hoặc đã được crawl).\")\n",
    "\n",
    "\n",
    "        current_page += 1\n",
    "        time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX)) # Sleep giữa các trang list\n",
    "\n",
    "    # --- Giai đoạn 2: Crawl chi tiết song song ---\n",
    "    if not all_products_basic_info:\n",
    "        print(\"Không có thông tin sản phẩm cơ bản nào được thu thập. Không thể crawl chi tiết.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n--- Giai đoạn 2: Thu thập chi tiết ({len(all_products_basic_info)} sản phẩm) (PARALLEL với max {MAX_CRAWL_WORKERS} workers) ---\")\n",
    "    all_products_detailed_list = [] # List chứa dict thông tin đầy đủ\n",
    "    total_to_crawl = len(all_products_basic_info)\n",
    "    completed_count = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_CRAWL_WORKERS) as executor:\n",
    "        futures = {executor.submit(get_product_details_v2, basic_info.get('product_url'), basic_info): basic_info\n",
    "                   for basic_info in all_products_basic_info if basic_info.get('product_url')}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            basic_info = futures[future]\n",
    "            try:\n",
    "                detailed_info = future.result()\n",
    "                final_product_info = basic_info.copy()\n",
    "                final_product_info.update(detailed_info)\n",
    "                all_products_detailed_list.append(final_product_info)\n",
    "\n",
    "            except Exception as exc:\n",
    "                print(f\"!!! Lỗi xảy ra khi xử lý future cho ID {basic_info.get('product_id')}: {exc}\")\n",
    "                error_info = basic_info.copy()\n",
    "                error_info['description'] = f\"Lỗi crawl parallel: {exc}\"\n",
    "                error_info['origin'] = error_info.get('origin', 'Lỗi crawl')\n",
    "                error_info['producer'] = error_info.get('producer', 'Lỗi crawl')\n",
    "                error_info['ocop_rating'] = error_info.get('ocop_rating', None) # pd.NA might be better for numeric later\n",
    "                all_products_detailed_list.append(error_info)\n",
    "\n",
    "            completed_count += 1\n",
    "            if completed_count % 50 == 0 or completed_count == total_to_crawl:\n",
    "                 print(f\"   Đã hoàn thành crawl chi tiết {completed_count}/{total_to_crawl} sản phẩm...\")\n",
    "\n",
    "    print(f\"\\n--- Hoàn tất crawl parallel ---\")\n",
    "    if not all_products_detailed_list:\n",
    "        print(\"Không crawl được thông tin chi tiết sản phẩm nào.\")\n",
    "        return None\n",
    "\n",
    "    detailed_map = {int(info['product_id']): info for info in all_products_detailed_list if info.get('product_id') is not None}\n",
    "    ordered_detailed_list = []\n",
    "    for basic in all_products_basic_info:\n",
    "        pid = basic.get('product_id')\n",
    "        if pid is not None:\n",
    "            detail = detailed_map.get(int(pid))\n",
    "            if detail:\n",
    "                ordered_detailed_list.append(detail)\n",
    "            # else:\n",
    "                # print(f\"   - Cảnh báo: Không tìm thấy thông tin chi tiết đã crawl cho ID {pid}\")\n",
    "\n",
    "    if not ordered_detailed_list:\n",
    "        print(\"Không có sản phẩm nào sau khi sắp xếp và lọc.\")\n",
    "        return None\n",
    "\n",
    "    df_crawled = pd.DataFrame(ordered_detailed_list)\n",
    "    final_columns_v2 = [\"product_id\", \"name\", \"full_name\", \"price\", \"ocop_rating\", \"origin\", \"producer\", \"short_description\", \"description\", \"image_url\", \"product_url\", \"ocop_rating_from_list\"]\n",
    "    for col in final_columns_v2:\n",
    "        if col not in df_crawled.columns:\n",
    "             if col == 'price': df_crawled[col] = 0\n",
    "             elif col in ['ocop_rating', 'ocop_rating_from_list']: df_crawled[col] = pd.NA\n",
    "             else: df_crawled[col] = ''\n",
    "    df_final = df_crawled[final_columns_v2].copy()\n",
    "    return df_final\n",
    "\n",
    "print(\"Block 2: Hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10cbf1-69a5-4b60-970f-2891ce0d587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Block 3: Tải/Crawl Dữ liệu Chính và Làm sạch Ban đầu - Đang chạy...\")\n",
    "\n",
    "# --- Hàm Tải/Crawl và Làm sạch ---\n",
    "def load_or_crawl_data(csv_path, use_parallel_crawl=True):\n",
    "    \"\"\"Tải dữ liệu từ CSV nếu có, ngược lại thì crawl. Trả về DataFrame và trạng thái crawl.\"\"\"\n",
    "    df = None\n",
    "    csv_existed_before = os.path.exists(csv_path) # Kiểm tra file tồn tại trước khi thử load\n",
    "\n",
    "    if csv_existed_before:\n",
    "        print(f\"Tìm thấy file CSV: {csv_path}. Đang tải...\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"Tải thành công {len(df)} sản phẩm từ CSV.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Lỗi: File CSV '{csv_path}' rỗng. Sẽ crawl lại.\")\n",
    "            df = None\n",
    "            csv_existed_before = False # Coi như file không tồn tại để rebuild model\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi đọc CSV: {e}. Sẽ crawl lại.\")\n",
    "            df = None\n",
    "            csv_existed_before = False\n",
    "    else:\n",
    "        print(f\"Không tìm thấy file CSV: {csv_path}. Sẽ crawl dữ liệu mới.\")\n",
    "\n",
    "    crawled_this_run = False # Flag để biết có crawl trong lần chạy này không\n",
    "    if df is None or df.empty: # Nếu không load được hoặc file rỗng\n",
    "        if use_parallel_crawl:\n",
    "             print(\"Tiến hành crawl dữ liệu mới (V2 - Parallel)...\")\n",
    "             df = run_crawl_v2_parallel() # Gọi hàm crawl song song\n",
    "        else:\n",
    "             print(\"Chế độ crawl tuần tự chưa được triển khai đầy đủ trong script này.\")\n",
    "             # df = run_crawl_v2() # Cần định nghĩa hàm run_crawl_v2() nếu muốn dùng\n",
    "             return None, False # Trả về None và chưa crawl\n",
    "\n",
    "        crawled_this_run = True # Đánh dấu là đã crawl\n",
    "        if df is not None and not df.empty:\n",
    "            try:\n",
    "                # Lưu dữ liệu mới crawl vào CSV\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "                print(f\"Đã lưu dữ liệu crawl mới vào: {csv_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi lưu file CSV mới: {e}.\")\n",
    "        else:\n",
    "            print(\"Lỗi: Crawl dữ liệu không thành công hoặc không trả về DataFrame.\")\n",
    "            return None, crawled_this_run # Trả về None nhưng vẫn đánh dấu đã thử crawl\n",
    "\n",
    "    # --- Làm sạch và Chuẩn hóa (Thực hiện trên df đã tải hoặc mới crawl) ---\n",
    "    if df is not None and not df.empty:\n",
    "        print(\"Đang làm sạch và chuẩn hóa dữ liệu...\")\n",
    "        original_row_count = len(df)\n",
    "        if 'product_id' not in df.columns:\n",
    "            print(\"LỖI nghiêm trọng: Thiếu cột 'product_id' trong DataFrame.\")\n",
    "            return None, crawled_this_run\n",
    "\n",
    "        # 1. Chuyển đổi product_id sang numeric, xử lý lỗi, loại bỏ NaN/invalid\n",
    "        df['product_id'] = pd.to_numeric(df['product_id'], errors='coerce')\n",
    "        df_cleaned = df.dropna(subset=['product_id']).copy() # Loại bỏ hàng có product_id là NaN và tạo bản sao\n",
    "        if df_cleaned.empty:\n",
    "            print(\"Lỗi: Không có product_id hợp lệ sau khi loại bỏ NaN.\")\n",
    "            return None, crawled_this_run\n",
    "        df_cleaned['product_id'] = df_cleaned['product_id'].astype(int) # Chuyển sang int\n",
    "\n",
    "        # 2. Loại bỏ ID nhiễu cụ thể\n",
    "        ids_to_remove = [20977, 20978] # ID của Rựa/Cuốc\n",
    "        initial_len_before_remove = len(df_cleaned)\n",
    "        df_filtered = df_cleaned[~df_cleaned['product_id'].isin(ids_to_remove)].copy() # Lọc và tạo bản sao\n",
    "        removed_count = initial_len_before_remove - len(df_filtered)\n",
    "        if removed_count > 0:\n",
    "            print(f\"Đã loại bỏ {removed_count} sản phẩm có ID nhiễu.\")\n",
    "\n",
    "        df_final = df_filtered\n",
    "        if df_final.empty:\n",
    "            print(\"LỖI: Không còn sản phẩm nào sau khi loại bỏ ID nhiễu.\")\n",
    "            return None, crawled_this_run\n",
    "\n",
    "        # 3. Chuẩn hóa kiểu dữ liệu các cột khác và xử lý NaN\n",
    "        num_cols = ['price', 'ocop_rating', 'ocop_rating_from_list']\n",
    "        for col in num_cols:\n",
    "             if col in df_final.columns:\n",
    "                 df_final[col] = pd.to_numeric(df_final[col], errors='coerce') # Chuyển sang numeric, lỗi thành NaN (pd.NA)\n",
    "\n",
    "        # Xử lý NaN cụ thể cho từng cột numeric\n",
    "        if 'price' in df_final.columns:\n",
    "            df_final['price'] = df_final['price'].fillna(0).astype(int) # Fill giá NaN = 0\n",
    "        if 'ocop_rating' in df_final.columns:\n",
    "            df_final['ocop_rating'] = df_final['ocop_rating'] # Giữ NaN (pd.NA) cho rating\n",
    "        if 'ocop_rating_from_list' in df_final.columns:\n",
    "             df_final['ocop_rating_from_list'] = df_final['ocop_rating_from_list'] # Giữ NaN (pd.NA)\n",
    "\n",
    "        text_cols = ['name', 'full_name', 'origin', 'producer', 'short_description', 'description', 'image_url', 'product_url']\n",
    "        for col in text_cols:\n",
    "             if col in df_final.columns:\n",
    "                 # Chuyển sang string, fill NaN bằng chuỗi rỗng\n",
    "                 df_final[col] = df_final[col].astype(str).fillna('')\n",
    "\n",
    "        # 4. Loại bỏ trùng lặp dựa trên product_id, giữ bản ghi đầu tiên\n",
    "        initial_count_before_dedup = len(df_final)\n",
    "        df_final = df_final.drop_duplicates(subset=['product_id'], keep='first')\n",
    "        duplicates_removed = initial_count_before_dedup - len(df_final)\n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"Đã loại bỏ {duplicates_removed} bản ghi trùng lặp dựa trên product_id.\")\n",
    "\n",
    "        # 5. Reset index sau khi lọc và loại bỏ trùng lặp\n",
    "        df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "        print(f\"Làm sạch hoàn tất. Từ {original_row_count} ban đầu còn lại {len(df_final)} sản phẩm hợp lệ.\")\n",
    "        # Trả về DataFrame đã xử lý và trạng thái crawl\n",
    "        return df_final, crawled_this_run\n",
    "    else:\n",
    "        # Trường hợp df là None hoặc empty ngay từ đầu\n",
    "        print(\"DataFrame đầu vào không hợp lệ hoặc rỗng.\")\n",
    "        return None, crawled_this_run\n",
    "\n",
    "\n",
    "# --- Thực thi Tải/Crawl ---\n",
    "# Đặt use_parallel_crawl=True để dùng crawl song song (khuyến nghị)\n",
    "df_processed, data_was_crawled = load_or_crawl_data(CSV_FILENAME, use_parallel_crawl=True)\n",
    "\n",
    "# Kiểm tra kết quả\n",
    "if df_processed is not None and not df_processed.empty:\n",
    "    print(\"\\nThông tin DataFrame đã xử lý:\")\n",
    "    df_processed.info()\n",
    "    # print(\"\\n5 dòng đầu:\")\n",
    "    # print(df_processed.head()) # Uncomment để xem thử dữ liệu\n",
    "else:\n",
    "    print(\"\\nKhông thể tải hoặc crawl dữ liệu. Các bước tiếp theo có thể sẽ lỗi.\")\n",
    "\n",
    "print(f\"Trạng thái crawl lần này: {data_was_crawled}\")\n",
    "print(\"Block 3: Hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715828d6-e4b0-47cb-89b1-84ea46b4e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Block 4: Định nghĩa Hàm Tạo Feature và Xây dựng Mô hình Nâng cao - Đang chạy...\")\n",
    "\n",
    "# --- Hàm Tạo Feature Tổng hợp (Phiên bản V2) ---\n",
    "def combine_features_v2(row):\n",
    "    \"\"\"Kết hợp các trường text thành một chuỗi duy nhất để tính TF-IDF.\"\"\"\n",
    "    # Lấy giá trị từ các cột, đảm bảo là string, xử lý NaN/None\n",
    "    name_part = str(row.get('name', '')) * NAME_WEIGHT # Nhân trọng số cho tên\n",
    "    origin_part = str(row.get('origin', ''))\n",
    "    producer_part = str(row.get('producer', ''))\n",
    "    description_part = str(row.get('description', ''))\n",
    "    short_desc_part = str(row.get('short_description', ''))\n",
    "\n",
    "    # Tạo list các phần text, chỉ bao gồm các chuỗi không rỗng và không phải 'nan'\n",
    "    feature_parts = [part for part in [name_part, origin_part, producer_part, short_desc_part, description_part]\n",
    "                     if isinstance(part, str) and part.strip() and part.strip().lower() != 'nan']\n",
    "\n",
    "    # Kết hợp các phần thành một chuỗi lớn\n",
    "    combined_text = \" \".join(feature_parts)\n",
    "\n",
    "    # Áp dụng từ đồng nghĩa trên chuỗi kết hợp\n",
    "    return apply_synonyms(combined_text)\n",
    "\n",
    "# --- Hàm Xây dựng Mô hình TF-IDF (Sử dụng tokenizer nâng cao và tham số tối ưu) ---\n",
    "def build_tfidf_model_advanced(df):\n",
    "    \"\"\"Xây dựng ma trận TF-IDF và Cosine Similarity từ DataFrame sản phẩm.\"\"\"\n",
    "    if df is None or df.empty or 'product_id' not in df.columns:\n",
    "        print(\"Lỗi: DataFrame không hợp lệ để xây dựng mô hình.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(\"\\n--- Bắt đầu Xây dựng mô hình TF-IDF Nâng cao ---\")\n",
    "\n",
    "    df_model = df.copy()\n",
    "    # 1. Chuẩn hóa product_id: Chuyển sang numeric để loại bỏ NaN, sau đó chuyển sang STRING\n",
    "    df_model['product_id'] = pd.to_numeric(df_model['product_id'], errors='coerce')\n",
    "    df_model = df_model.dropna(subset=['product_id'])\n",
    "    if df_model.empty:\n",
    "        print(\"LỖI: Không có product_id hợp lệ sau khi loại bỏ NaN.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # >>> SỬA Ở ĐÂY: Chuyển product_id thành STRING <<<\n",
    "    df_model['product_id'] = df_model['product_id'].astype(int).astype(str) # Chuyển sang int rồi mới sang str để loại bỏ '.0' nếu có\n",
    "\n",
    "    df_model = df_model.reset_index(drop=True)\n",
    "\n",
    "    # ... (phần tạo combined_features, TF-IDF, Cosine Similarity giữ nguyên) ...\n",
    "    print(\"Đang tạo cột 'combined_features' (v2)...\")\n",
    "    df_model['combined_features'] = df_model.apply(combine_features_v2, axis=1)\n",
    "    print(\"Tạo 'combined_features' hoàn tất.\")\n",
    "\n",
    "    print(\"Đang khởi tạo TfidfVectorizer Nâng cao...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        tokenizer=advanced_tokenizer,\n",
    "        ngram_range=TFIDF_NGRAM_RANGE,\n",
    "        min_df=TFIDF_MIN_DF,\n",
    "        max_df=TFIDF_MAX_DF,\n",
    "        max_features=TFIDF_MAX_FEATURES,\n",
    "        sublinear_tf=TFIDF_SUBLINEAR_TF,\n",
    "        norm=TFIDF_NORM\n",
    "    )\n",
    "    print(\"Đang tính toán ma trận TF-IDF...\")\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df_model['combined_features'])\n",
    "    print(f\"Kích thước ma trận TF-IDF: {tfidf_matrix.shape}\")\n",
    "\n",
    "    if tfidf_matrix.shape[0] == 0 or tfidf_matrix.shape[1] == 0:\n",
    "        print(\"LỖI: Ma trận TF-IDF rỗng!\")\n",
    "        return None, None, df_model\n",
    "\n",
    "    print(\"Đang tính toán ma trận Cosine Similarity...\")\n",
    "    cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    print(f\"Kích thước ma trận Cosine Similarity: {cosine_sim_matrix.shape}\")\n",
    "\n",
    "    # 5. Tạo Map từ product_id (STRING) sang index của DataFrame (df_model)\n",
    "    # Index của Series bây giờ sẽ là product_id dạng STRING\n",
    "    indices = pd.Series(df_model.index, index=df_model['product_id']).drop_duplicates()\n",
    "    print(f\"Tạo ánh xạ product_id (string) sang index hoàn tất. Số lượng ánh xạ: {len(indices)}\")\n",
    "    # Kiểm tra kiểu dữ liệu của index của Series 'indices'\n",
    "    # print(f\"DEBUG: Kiểu dữ liệu của index trong 'indices' Series: {indices.index.dtype}\")\n",
    "\n",
    "\n",
    "    return cosine_sim_matrix, indices, df_model\n",
    "\n",
    "print(\"Block 4: Hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb77a7c-b95a-4154-a65b-d0d475fd1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Block 5: Thực thi Xây dựng Mô hình Nâng cao và Lưu trữ JSON - Đang chạy...\")\n",
    "\n",
    "cosine_sim_matrix = None\n",
    "indices = None # Đây sẽ là Pandas Series với index là product_id (string)\n",
    "df_for_map_creation = None\n",
    "\n",
    "# Chỉ thực hiện nếu df_processed từ Block 3 hợp lệ và không rỗng\n",
    "if 'df_processed' in locals() and isinstance(df_processed, pd.DataFrame) and not df_processed.empty:\n",
    "\n",
    "    should_rebuild_model = True\n",
    "\n",
    "    # Kiểm tra xem có cần xây dựng lại model không\n",
    "    if not data_was_crawled and os.path.exists(COSINE_SIM_MATRIX_FILE) and os.path.exists(INDICES_MAP_FILE):\n",
    "        print(\"Kiểm tra file ma trận và indices đã lưu (do không crawl lần này)...\")\n",
    "        try:\n",
    "            temp_cosine_matrix = np.load(COSINE_SIM_MATRIX_FILE)\n",
    "            with open(INDICES_MAP_FILE, 'rb') as f:\n",
    "                temp_indices = pickle.load(f) # temp_indices là Pandas Series, index là product_id (string)\n",
    "\n",
    "            # Đảm bảo temp_indices là Series và index của nó là string\n",
    "            if not isinstance(temp_indices, pd.Series):\n",
    "                print(\"Lỗi: File indices đã lưu không phải là Pandas Series. Sẽ xây dựng lại.\")\n",
    "                raise ValueError(\"Saved indices is not a Pandas Series.\")\n",
    "            temp_indices.index = temp_indices.index.astype(str) # Đảm bảo index là string\n",
    "\n",
    "\n",
    "            # Chuẩn hóa product_id trong df_processed để so sánh\n",
    "            # Chuyển sang numeric để bỏ NaN, sau đó int, rồi string\n",
    "            df_processed_check = df_processed.copy()\n",
    "            df_processed_check['product_id_str_check'] = pd.to_numeric(df_processed_check['product_id'], errors='coerce')\n",
    "            df_processed_check.dropna(subset=['product_id_str_check'], inplace=True)\n",
    "\n",
    "            valid_df_count = 0\n",
    "            if not df_processed_check.empty:\n",
    "                 df_processed_check['product_id_str_check'] = df_processed_check['product_id_str_check'].astype(int).astype(str)\n",
    "                 valid_df_count = len(df_processed_check['product_id_str_check'].unique())\n",
    "\n",
    "            # So sánh số lượng product_id duy nhất\n",
    "            # temp_indices.index chứa các product_id (string)\n",
    "            if len(temp_indices.index.unique()) == valid_df_count and \\\n",
    "               temp_cosine_matrix.shape[0] == len(temp_indices.index.unique()):\n",
    "                print(f\"Kích thước khớp ({valid_df_count}). Sử dụng ma trận/indices đã lưu.\")\n",
    "                cosine_sim_matrix = temp_cosine_matrix\n",
    "                indices = temp_indices # indices này có index là product_id (string)\n",
    "                should_rebuild_model = False\n",
    "                df_for_map_creation = df_processed.copy()\n",
    "            else:\n",
    "                print(f\"Kích thước không khớp (Indices đã lưu: {len(temp_indices.index.unique()) if isinstance(temp_indices, pd.Series) else 'N/A'}, Matrix: {temp_cosine_matrix.shape[0]}, DataFrame hợp lệ: {valid_df_count}). Sẽ xây dựng lại mô hình...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi tải hoặc kiểm tra file ma trận/indices đã lưu: {e}. Sẽ xây dựng lại mô hình...\")\n",
    "            traceback.print_exc() # In traceback để debug\n",
    "    elif data_was_crawled:\n",
    "        print(\"Dữ liệu mới đã được crawl trong lần chạy này. Sẽ xây dựng lại mô hình.\")\n",
    "    else:\n",
    "         print(\"Không có file ma trận/indices đã lưu và không crawl mới. Sẽ xây dựng mô hình.\")\n",
    "\n",
    "    if should_rebuild_model:\n",
    "        # df_processed ở đây là DataFrame gốc từ Block 3\n",
    "        # Hàm build_tfidf_model_advanced sẽ xử lý việc chuyển product_id sang string bên trong nó\n",
    "        temp_cosine_sim_matrix, temp_indices, df_model_updated = build_tfidf_model_advanced(df_processed)\n",
    "\n",
    "        if temp_cosine_sim_matrix is not None and temp_indices is not None and df_model_updated is not None:\n",
    "            print(\"Xây dựng mô hình thành công.\")\n",
    "            cosine_sim_matrix = temp_cosine_sim_matrix\n",
    "            indices = temp_indices # indices này có index là product_id (string)\n",
    "            df_for_map_creation = df_model_updated.copy() # Dùng df đã được xử lý bởi build_tfidf_model_advanced\n",
    "\n",
    "            try:\n",
    "                print(\"Đang lưu trữ ma trận cosine và map indices mới...\")\n",
    "                np.save(COSINE_SIM_MATRIX_FILE, cosine_sim_matrix)\n",
    "                with open(INDICES_MAP_FILE, 'wb') as f:\n",
    "                    pickle.dump(indices, f) # Lưu Series indices\n",
    "                print(f\"Đã lưu thành công: {COSINE_SIM_MATRIX_FILE}, {INDICES_MAP_FILE}\")\n",
    "            except Exception as e:\n",
    "                print(f\"LỖI nghiêm trọng khi lưu file ma trận/indices: {e}\")\n",
    "                traceback.print_exc()\n",
    "                cosine_sim_matrix = None # Reset nếu không lưu được\n",
    "                indices = None\n",
    "        else:\n",
    "            print(\"LỖI: Không thể xây dựng mô hình TF-IDF/Cosine Similarity. Bỏ qua các bước sau.\")\n",
    "            cosine_sim_matrix = None\n",
    "            indices = None\n",
    "            df_for_map_creation = None # Không có df để tạo map\n",
    "\n",
    "    # --- Tạo và Lưu Map Sản phẩm (PRODUCT_MAP_JSON_FILE) ---\n",
    "    if df_for_map_creation is not None and not df_for_map_creation.empty and 'product_id' in df_for_map_creation.columns:\n",
    "        print(\"\\nĐang tạo và lưu Map thông tin sản phẩm (JSON)...\")\n",
    "        try:\n",
    "            map_columns = ['product_id', 'name', 'origin', 'producer', 'image_url',\n",
    "                           'price', 'ocop_rating', 'product_url', 'full_name',\n",
    "                           'category', 'short_description', 'description', 'num_reviews', 'sold'] # Thêm các trường cần thiết\n",
    "\n",
    "            existing_map_columns = [col for col in map_columns if col in df_for_map_creation.columns]\n",
    "            df_map = df_for_map_creation[existing_map_columns].copy()\n",
    "\n",
    "            # 1. Chuẩn hóa product_id sang string để làm key JSON (đã được làm trong build_tfidf_model_advanced nếu model được build lại)\n",
    "            # Nếu model được load từ file, df_for_map_creation là df_processed, cần chuẩn hóa ở đây\n",
    "            if 'product_id' in df_map.columns: # Kiểm tra lại\n",
    "                df_map['product_id_numeric'] = pd.to_numeric(df_map['product_id'], errors='coerce')\n",
    "                df_map.dropna(subset=['product_id_numeric'], inplace=True)\n",
    "                if not df_map.empty:\n",
    "                    df_map['product_id'] = df_map['product_id_numeric'].astype(int).astype(str)\n",
    "                df_map.drop(columns=['product_id_numeric'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "            # 2. Xử lý NaN/NA và đảm bảo kiểu dữ liệu đúng cho JSON\n",
    "            for col in ['name', 'origin', 'producer', 'image_url', 'product_url', 'full_name', 'category', 'short_description', 'description']:\n",
    "                if col in df_map.columns:\n",
    "                    df_map[col] = df_map[col].fillna('').astype(str)\n",
    "\n",
    "            for col in ['price', 'ocop_rating', 'num_reviews', 'sold']: # Thêm num_reviews, sold\n",
    "                if col in df_map.columns:\n",
    "                    df_map[col] = pd.to_numeric(df_map[col], errors='coerce') # Chuyển sang số\n",
    "                    # Đối với rating, num_reviews, sold có thể muốn fillna bằng 0 hoặc giữ NA rồi xử lý khi dump JSON\n",
    "                    if col == 'price':\n",
    "                        df_map[col] = df_map[col].fillna(0).astype(int)\n",
    "                    elif col == 'ocop_rating':\n",
    "                         # Chuyển float NaN thành None, giữ nguyên số nguyên/float khác\n",
    "                        df_map[col] = df_map[col].apply(lambda x: safe_int_convert(x) if pd.notnull(x) else None)\n",
    "                    elif col in ['num_reviews', 'sold']:\n",
    "                         df_map[col] = df_map[col].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "            # 3. Tạo dictionary với product_id (string) làm key\n",
    "            if df_map.empty or 'product_id' not in df_map.columns or df_map['product_id'].isnull().all():\n",
    "                print(\"Lỗi: Không có product_id hợp lệ trong df_map để tạo product_map_dict.\")\n",
    "                product_map_dict = {}\n",
    "            else:\n",
    "                # Loại bỏ các hàng có product_id là NaN/None một lần nữa trước khi set_index\n",
    "                df_map.dropna(subset=['product_id'], inplace=True)\n",
    "                # Đảm bảo product_id là duy nhất trước khi set_index, giữ bản ghi đầu tiên nếu trùng\n",
    "                df_map.drop_duplicates(subset=['product_id'], keep='first', inplace=True)\n",
    "                if not df_map.empty:\n",
    "                    product_map_dict = df_map.set_index('product_id').to_dict('index')\n",
    "                else:\n",
    "                    product_map_dict = {}\n",
    "\n",
    "\n",
    "            # 4. Lưu dictionary vào file JSON\n",
    "            with open(PRODUCT_MAP_JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "                 json.dump(product_map_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"Đã lưu map sản phẩm vào: {PRODUCT_MAP_JSON_FILE} với {len(product_map_dict)} sản phẩm.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi tạo hoặc lưu file map sản phẩm JSON: {e}\")\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Không có DataFrame hợp lệ ('df_for_map_creation') để tạo map sản phẩm.\")\n",
    "\n",
    "else:\n",
    "    print(\"Không có dữ liệu 'df_processed' từ Block 3 để thực hiện Block 5.\")\n",
    "\n",
    "print(\"Block 5: Hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cd8a8-ee72-4127-9727-8f04014bb12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Block 6: Định nghĩa Hàm Gợi ý và Tiền tính toán - Đang chạy...\")\n",
    "\n",
    "# --- Hàm Gợi ý ID Sản phẩm (Dùng cho tiền tính toán) ---\n",
    "def get_recommendation_ids_for_precomputation(product_id_int, indices_map, cosine_matrix, index_to_id_map, top_n=30):\n",
    "    \"\"\"Lấy list các ID sản phẩm gợi ý (dạng int) cho một ID đầu vào.\"\"\"\n",
    "    if product_id_int not in indices_map:\n",
    "        # print(f\"  Debug: ID {product_id_int} không có trong indices_map.\")\n",
    "        return [] # Trả về list rỗng nếu ID không có trong map\n",
    "\n",
    "    idx = indices_map[product_id_int] # Lấy DataFrame index từ product_id\n",
    "\n",
    "    # Kiểm tra index hợp lệ so với kích thước ma trận\n",
    "    if idx >= cosine_matrix.shape[0]:\n",
    "        # print(f\"  Debug: Chỉ số DataFrame ({idx}) cho ID {product_id_int} vượt quá kích thước ma trận ({cosine_matrix.shape[0]}).\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Lấy hàng tương ứng trong ma trận cosine, kèm theo index gốc\n",
    "        sim_scores_with_indices = list(enumerate(cosine_matrix[idx]))\n",
    "\n",
    "        # Sắp xếp theo điểm số giảm dần\n",
    "        sim_scores_with_indices = sorted(sim_scores_with_indices, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Lấy top_n+1 gợi ý (bao gồm cả chính nó) và bỏ đi gợi ý đầu tiên (chính nó)\n",
    "        top_recommendations_indices = [score_tuple[0] for score_tuple in sim_scores_with_indices[1 : top_n + 1]]\n",
    "\n",
    "        # Chuyển đổi DataFrame indices thành product_ids (int) dùng map ngược\n",
    "        recommended_ids_int = [index_to_id_map.get(rec_idx) for rec_idx in top_recommendations_indices]\n",
    "        # Lọc bỏ các giá trị None (nếu có lỗi trong map ngược)\n",
    "        recommended_ids_int = [pid for pid in recommended_ids_int if pid is not None]\n",
    "\n",
    "        return recommended_ids_int # Trả về list các product ID (integer)\n",
    "\n",
    "    except IndexError:\n",
    "         print(f\"Lỗi IndexError khi truy cập cosine_matrix[{idx}] cho ID {product_id_int}.\")\n",
    "         return []\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định khi lấy gợi ý cho ID {product_id_int}: {e}\")\n",
    "        # traceback.print_exc() # Uncomment để debug\n",
    "        return []\n",
    "\n",
    "\n",
    "# --- Hàm Tiền tính toán Gợi ý (Thô - JSON) ---\n",
    "def precompute_raw_recommendations_json(all_product_ids_int, indices_map, cosine_matrix, top_n=30):\n",
    "    \"\"\"Tiền tính toán và trả về dict {\"product_id_str\": [rec_id_int_1, rec_id_int_2,...]}.\"\"\"\n",
    "    precomputed_recs_dict = {} # Dict để lưu kết quả\n",
    "    total_products = len(all_product_ids_int)\n",
    "    print(f\"\\n--- Bắt đầu Tiền tính toán Top {top_n} gợi ý THÔ cho {total_products} sản phẩm ---\")\n",
    "\n",
    "    # Tạo map ngược từ DataFrame index -> product_id (int) để tra cứu nhanh\n",
    "    try:\n",
    "        index_to_id_map = {v: k for k, v in indices_map.items()}\n",
    "        print(f\"Đã tạo map ngược index -> product_id thành công ({len(index_to_id_map)} mục).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tạo map ngược index->ID: {e}. Việc tra cứu ID gợi ý có thể chậm hơn.\")\n",
    "        index_to_id_map = None # Sẽ phải duyệt indices_map nếu không tạo được map ngược\n",
    "\n",
    "    processed_count = 0\n",
    "    # Duyệt qua từng product ID cần tính toán\n",
    "    for prod_id_int in all_product_ids_int:\n",
    "        # Gọi hàm lấy list ID gợi ý cho ID hiện tại\n",
    "        recommended_ids = get_recommendation_ids_for_precomputation(\n",
    "            prod_id_int,\n",
    "            indices_map,\n",
    "            cosine_matrix,\n",
    "            index_to_id_map, # Truyền map ngược vào\n",
    "            top_n=top_n\n",
    "        )\n",
    "\n",
    "        # Lưu kết quả vào dict, với key là product ID dạng string\n",
    "        precomputed_recs_dict[str(prod_id_int)] = recommended_ids\n",
    "\n",
    "        processed_count += 1\n",
    "        # In tiến trình\n",
    "        if processed_count % 200 == 0 or processed_count == total_products:\n",
    "            print(f\"  Đã xử lý tiền tính toán cho {processed_count}/{total_products} sản phẩm...\")\n",
    "\n",
    "    print(f\"--- Tiền tính toán gợi ý thô hoàn tất ({len(precomputed_recs_dict)} sản phẩm có gợi ý) ---\")\n",
    "    return precomputed_recs_dict\n",
    "\n",
    "# --- Hàm Lấy Gợi ý Chi tiết (Dùng cho kiểm tra cuối cùng) ---\n",
    "def get_final_recommendations_with_details(product_id_int, precomputed_recs_dict, product_map_dict, top_n=10):\n",
    "    \"\"\"Lấy thông tin chi tiết cho các ID gợi ý đã được tiền tính toán.\"\"\"\n",
    "    product_id_str = str(product_id_int)\n",
    "    recommended_ids_int = precomputed_recs_dict.get(product_id_str, [])\n",
    "\n",
    "    if not recommended_ids_int:\n",
    "        # print(f\"Không có gợi ý tiền tính toán cho ID {product_id_int}.\")\n",
    "        return []\n",
    "\n",
    "    recommendations_details = []\n",
    "    # Lấy tối đa top_n gợi ý đầu tiên từ list đã tiền tính toán\n",
    "    for rec_id_int in recommended_ids_int[:top_n]:\n",
    "        rec_id_str = str(rec_id_int)\n",
    "        rec_info = product_map_dict.get(rec_id_str) # Lấy thông tin từ map sản phẩm\n",
    "        if rec_info:\n",
    "            # Tạo một bản sao để tránh thay đổi dict gốc\n",
    "            detail = rec_info.copy()\n",
    "            detail['product_id'] = rec_id_int # Đảm bảo product_id là int trong kết quả cuối\n",
    "            # Score không có sẵn ở đây vì chỉ lưu ID, cần tính lại nếu muốn hiển thị score\n",
    "            # detail['similarity_score'] = ... # Cần tính lại nếu muốn score\n",
    "            recommendations_details.append(detail)\n",
    "        # else:\n",
    "            # print(f\"  Cảnh báo: ID gợi ý {rec_id_int} không tìm thấy trong product_map_dict.\")\n",
    "\n",
    "    return recommendations_details\n",
    "\n",
    "\n",
    "print(\"Block 6: Hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633079e-275b-480e-9100-5ff8c58fd7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Block 7: Thực thi Tiền tính toán và Lưu Gợi ý JSON - Đang chạy...\")\n",
    "\n",
    "precomputed_recommendations_dict = None # Dict chứa kết quả tiền tính toán\n",
    "\n",
    "# Chỉ thực hiện nếu ma trận và indices đã được tạo/tải thành công ở Block 5\n",
    "if 'indices' in locals() and indices is not None and \\\n",
    "   'cosine_sim_matrix' in locals() and cosine_sim_matrix is not None:\n",
    "\n",
    "    print(f\"Dữ liệu đầu vào cho tiền tính toán: Indices({len(indices)}), Matrix({cosine_sim_matrix.shape})\")\n",
    "    try:\n",
    "        # Lấy danh sách các product ID duy nhất (dạng int) từ index của Series 'indices'\n",
    "        all_product_ids_to_precompute = indices.index.unique().tolist()\n",
    "        print(f\"Số lượng ID sản phẩm duy nhất cần tiền tính toán: {len(all_product_ids_to_precompute)}\")\n",
    "\n",
    "        if not all_product_ids_to_precompute:\n",
    "             print(\"LỖI: Không có ID sản phẩm nào để tiền tính toán.\")\n",
    "        else:\n",
    "            # Gọi hàm tiền tính toán gợi ý (sử dụng TOP_N_RAW_RECS)\n",
    "            precomputed_recommendations_dict = precompute_raw_recommendations_json(\n",
    "                all_product_ids_to_precompute,\n",
    "                indices,               # Map product_id (int) -> df_index (int)\n",
    "                cosine_sim_matrix,     # Ma trận cosine similarity\n",
    "                top_n=TOP_N_RAW_RECS   # Số lượng gợi ý thô cần lưu cho mỗi sản phẩm\n",
    "            )\n",
    "\n",
    "            # Lưu kết quả tiền tính toán vào file JSON\n",
    "            if precomputed_recommendations_dict: # Chỉ lưu nếu có kết quả\n",
    "                print(f\"Đang lưu gợi ý tiền tính toán vào: {PRECOMPUTED_RECS_JSON_FILE}\")\n",
    "                try:\n",
    "                    with open(PRECOMPUTED_RECS_JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "                        # Lưu dict {\"product_id_str\": [rec_id_int_1, rec_id_int_2,...]}\n",
    "                        json.dump(precomputed_recommendations_dict, f, ensure_ascii=False, indent=2)\n",
    "                    print(f\"Đã lưu thành công {len(precomputed_recommendations_dict)} sản phẩm vào {PRECOMPUTED_RECS_JSON_FILE}\")\n",
    "                except Exception as save_err:\n",
    "                    print(f\"LỖI khi lưu file gợi ý JSON: {save_err}\")\n",
    "            else:\n",
    "                 print(\"Không có kết quả gợi ý nào được tạo ra để lưu.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi nghiêm trọng trong quá trình tiền tính toán và lưu gợi ý: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Thiếu dữ liệu đầu vào (indices hoặc cosine_sim_matrix) từ Block 5. Không thể tiền tính toán gợi ý.\")\n",
    "\n",
    "print(\"Block 7: Hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3eba66-cd06-4360-8600-5846f8963d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- BLOCK 8: KIỂM TRA GỢI Ý CUỐI CÙNG (SỬ DỤNG FILE JSON VÀ TÍNH LẠI SCORE) ---\n",
    "# ==============================================================================\n",
    "print(\"\\nBlock 8: Kiểm tra Gợi ý Cuối cùng (Sử dụng file JSON và tính lại score) - Đang chạy...\")\n",
    "\n",
    "# --- ĐỊNH NGHĨA LẠI HOẶC ĐẢM BẢO CÁC HÀM TIỆN ÍCH ĐÃ CÓ ---\n",
    "# (Nếu các hàm này đã được định nghĩa ở block trước và chạy đúng, bạn không cần định nghĩa lại)\n",
    "def safe_int_convert(value):\n",
    "    try: return int(float(value)) # Chuyển qua float trước để xử lý \"3.0\"\n",
    "    except (ValueError, TypeError, OverflowError): return None\n",
    "\n",
    "def safe_float_convert(value):\n",
    "    try: return float(value)\n",
    "    except (ValueError, TypeError, OverflowError): return None\n",
    "# --- KẾT THÚC ĐỊNH NGHĨA HÀM TIỆN ÍCH ---\n",
    "\n",
    "\n",
    "# --- HÀM LẤY GỢI Ý CHI TIẾT VÀ SCORE ---\n",
    "def get_final_recommendations_with_details_and_scores(\n",
    "    product_id_input_str,\n",
    "    precomputed_recs_dict,\n",
    "    product_map_dict,\n",
    "    cosine_sim_matrix_loaded,\n",
    "    indices_map_loaded_series,\n",
    "    top_n=TOP_N_FINAL_RECS # TOP_N_FINAL_RECS cần được định nghĩa ở scope này hoặc global\n",
    "):\n",
    "    recommended_ids_int_list = precomputed_recs_dict.get(product_id_input_str)\n",
    "\n",
    "    if not recommended_ids_int_list:\n",
    "        return []\n",
    "\n",
    "    recommendations_with_details = []\n",
    "\n",
    "    if product_id_input_str not in indices_map_loaded_series.index:\n",
    "        return []\n",
    "    original_product_matrix_idx = indices_map_loaded_series[product_id_input_str]\n",
    "\n",
    "    if not isinstance(original_product_matrix_idx, (int, np.integer)):\n",
    "        return []\n",
    "\n",
    "    for rec_id_int in recommended_ids_int_list[:top_n]:\n",
    "        rec_id_str = str(rec_id_int)\n",
    "        rec_product_details = product_map_dict.get(rec_id_str)\n",
    "\n",
    "        if rec_product_details:\n",
    "            score = 0.0\n",
    "            if rec_id_str in indices_map_loaded_series.index:\n",
    "                recommended_product_matrix_idx = indices_map_loaded_series[rec_id_str]\n",
    "                if isinstance(recommended_product_matrix_idx, (int, np.integer)):\n",
    "                    if (0 <= original_product_matrix_idx < cosine_sim_matrix_loaded.shape[0] and\n",
    "                        0 <= recommended_product_matrix_idx < cosine_sim_matrix_loaded.shape[1]):\n",
    "                        score = cosine_sim_matrix_loaded[original_product_matrix_idx, recommended_product_matrix_idx]\n",
    "\n",
    "            recommendations_with_details.append({\n",
    "                \"product_id\": rec_id_int,\n",
    "                \"name\": rec_product_details.get('name', 'N/A'),\n",
    "                \"origin\": rec_product_details.get('origin', 'N/A'),\n",
    "                \"producer\": rec_product_details.get('producer', 'N/A'),\n",
    "                \"price\": safe_float_convert(rec_product_details.get('price', 0)), # Đã có thể gọi\n",
    "                \"ocop_rating\": safe_int_convert(rec_product_details.get('ocop_rating')), # Đã có thể gọi\n",
    "                \"image_url\": rec_product_details.get('image_url', ''),\n",
    "                \"product_url\": rec_product_details.get('product_url', ''),\n",
    "                \"similarity_score\": float(score) if pd.notnull(score) else 0.0\n",
    "            })\n",
    "    return recommendations_with_details\n",
    "\n",
    "# --- KẾT THÚC HÀM LẤY GỢI Ý ---\n",
    "\n",
    "\n",
    "# Kiểm tra sự tồn tại của các file\n",
    "map_exists = os.path.exists(PRODUCT_MAP_JSON_FILE)\n",
    "recs_exists = os.path.exists(PRECOMPUTED_RECS_JSON_FILE)\n",
    "matrix_exists = os.path.exists(COSINE_SIM_MATRIX_FILE)\n",
    "indices_file_exists = os.path.exists(INDICES_MAP_FILE)\n",
    "\n",
    "if map_exists and recs_exists and matrix_exists and indices_file_exists:\n",
    "    print(\"\\n--- KIỂM TRA GỢI Ý CUỐI CÙNG (TỪ FILE JSON, TÍNH LẠI SCORE) ---\")\n",
    "    product_map_loaded_dict = None\n",
    "    precomputed_recs_loaded_dict = None\n",
    "    cosine_sim_matrix_loaded = None\n",
    "    indices_map_loaded_series = None # Đổi tên biến cho rõ đây là Series\n",
    "\n",
    "    try:\n",
    "        print(f\"Đang tải map sản phẩm từ JSON: {PRODUCT_MAP_JSON_FILE}\")\n",
    "        with open(PRODUCT_MAP_JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "             product_map_loaded_dict = json.load(f)\n",
    "        print(f\"Tải map sản phẩm thành công ({len(product_map_loaded_dict)} sản phẩm).\")\n",
    "\n",
    "        print(f\"Đang tải gợi ý tiền tính toán từ JSON: {PRECOMPUTED_RECS_JSON_FILE}\")\n",
    "        with open(PRECOMPUTED_RECS_JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "            precomputed_recs_loaded_dict = json.load(f)\n",
    "        print(f\"Tải gợi ý tiền tính toán thành công ({len(precomputed_recs_loaded_dict)} sản phẩm có gợi ý).\")\n",
    "\n",
    "        print(f\"Đang tải ma trận cosine similarity từ: {COSINE_SIM_MATRIX_FILE}\")\n",
    "        cosine_sim_matrix_loaded = np.load(COSINE_SIM_MATRIX_FILE)\n",
    "        print(f\"Tải ma trận thành công (shape: {cosine_sim_matrix_loaded.shape}).\")\n",
    "\n",
    "        print(f\"Đang tải map product ID sang index (Pandas Series) từ: {INDICES_MAP_FILE}\")\n",
    "        with open(INDICES_MAP_FILE, 'rb') as f:\n",
    "            indices_map_loaded_series = pickle.load(f) # Load Pandas Series\n",
    "\n",
    "        if not isinstance(indices_map_loaded_series, pd.Series):\n",
    "            print(f\"LỖI: File {INDICES_MAP_FILE} không chứa đối tượng Pandas Series hợp lệ.\")\n",
    "            indices_map_loaded_series = None # Đặt là None nếu không đúng\n",
    "        elif indices_map_loaded_series.index.dtype != 'object': # Kiểm tra kiểu index, nên là object (cho string)\n",
    "             # Chuyển đổi index của Series thành string nếu nó chưa phải\n",
    "             indices_map_loaded_series.index = indices_map_loaded_series.index.astype(str)\n",
    "             print(f\"Tải map index (Pandas Series) thành công ({len(indices_map_loaded_series)} entries). Index được chuyển thành string.\")\n",
    "        else:\n",
    "             print(f\"Tải map index (Pandas Series) thành công ({len(indices_map_loaded_series)} entries). Index type: {indices_map_loaded_series.index.dtype}.\")\n",
    "\n",
    "\n",
    "        if product_map_loaded_dict and precomputed_recs_loaded_dict and \\\n",
    "           cosine_sim_matrix_loaded is not None and indices_map_loaded_series is not None and not indices_map_loaded_series.empty:\n",
    "\n",
    "            available_ids_in_recs_str = list(precomputed_recs_loaded_dict.keys()) # Đây là các product_id_str\n",
    "\n",
    "            if not available_ids_in_recs_str:\n",
    "                print(\"LỖI: Không có ID nào trong file gợi ý đã tải (precomputed_recs_loaded_dict).\")\n",
    "            else:\n",
    "                valid_ids_for_testing_str = []\n",
    "                for id_str_candidate in available_ids_in_recs_str:\n",
    "                    # id_str_candidate đã là string từ key của precomputed_recs_loaded_dict\n",
    "                    # Kiểm tra xem id_str_candidate có trong index của indices_map_loaded_series không\n",
    "                    # và có trong product_map_loaded_dict (dict) không\n",
    "                    if id_str_candidate in indices_map_loaded_series.index and \\\n",
    "                       id_str_candidate in product_map_loaded_dict:\n",
    "                        valid_ids_for_testing_str.append(id_str_candidate)\n",
    "\n",
    "                if not valid_ids_for_testing_str:\n",
    "                    print(\"LỖI: Không có ID nào từ precomputed_recs hợp lệ để kiểm tra (không khớp giữa precomputed_recs, indices_map và product_map).\")\n",
    "                else:\n",
    "                    num_random_samples = min(5, len(valid_ids_for_testing_str)) # Tránh lỗi nếu valid_ids < 5\n",
    "                    ids_to_test_str = random.sample(valid_ids_for_testing_str, num_random_samples)\n",
    "                    print(f\"Chọn ngẫu nhiên {num_random_samples} ID hợp lệ để kiểm tra: {ids_to_test_str}\")\n",
    "\n",
    "                    for test_product_id_str in ids_to_test_str:\n",
    "                        print(f\"\\n--- Đang kiểm tra cho ID_STR: {test_product_id_str} ---\")\n",
    "                        original_product_info = product_map_loaded_dict.get(test_product_id_str)\n",
    "\n",
    "                        if not original_product_info:\n",
    "                            print(f\"  Cảnh báo: Không tìm thấy thông tin sản phẩm gốc cho ID_STR {test_product_id_str} trong product_map.\")\n",
    "                            continue\n",
    "\n",
    "                        print(\"=\"*50)\n",
    "                        print(f\"Sản phẩm Gốc (ID_STR: {test_product_id_str})\") # In ID string\n",
    "                        print(f\"  Tên: {original_product_info.get('name', 'N/A')}\")\n",
    "                        print(f\"  Xuất xứ: {original_product_info.get('origin', 'N/A')}\")\n",
    "                        print(f\"  Nhà sản xuất: {original_product_info.get('producer', 'N/A')}\")\n",
    "                        print(\"-\" * 20)\n",
    "                        print(f\"Gợi ý (Top {TOP_N_FINAL_RECS}):\")\n",
    "\n",
    "                        recommendations_details = get_final_recommendations_with_details_and_scores(\n",
    "                            test_product_id_str, # Truyền product_id dạng string\n",
    "                            precomputed_recs_loaded_dict,\n",
    "                            product_map_loaded_dict,\n",
    "                            cosine_sim_matrix_loaded,\n",
    "                            indices_map_loaded_series,\n",
    "                            top_n=TOP_N_FINAL_RECS\n",
    "                        )\n",
    "\n",
    "                        if recommendations_details:\n",
    "                            print(f\"  Tìm thấy {len(recommendations_details)} gợi ý chi tiết.\")\n",
    "                            for i, rec_detail in enumerate(recommendations_details):\n",
    "                                score_display = f\"{rec_detail.get('similarity_score', 0.0):.4f}\"\n",
    "                                print(f\"  {i+1}. ID: {rec_detail.get('product_id','Lỗi ID')} (Score: {score_display})\")\n",
    "                                print(f\"     Tên: {rec_detail.get('name', 'N/A')}\")\n",
    "                        else:\n",
    "                            print(f\"  - Không có gợi ý chi tiết nào được tìm thấy cho ID này.\")\n",
    "                        print(\"=\"*50)\n",
    "        else:\n",
    "            print(\"Một hoặc nhiều file artifact cần thiết không được tải hoặc rỗng.\")\n",
    "\n",
    "    except FileNotFoundError as e: print(f\"\\nLỖI: Không tìm thấy file cần thiết: {e}\")\n",
    "    except json.JSONDecodeError as e: print(f\"\\nLỖI: File JSON không hợp lệ: {e}\")\n",
    "    except pickle.UnpicklingError as e: print(f\"\\nLỖI: File Pickle không hợp lệ: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nLỖI không xác định: {type(e).__name__} - {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nKhông thể kiểm tra gợi ý do thiếu file:\")\n",
    "    if not map_exists: print(f\" - Thiếu: {PRODUCT_MAP_JSON_FILE}\")\n",
    "    if not recs_exists: print(f\" - Thiếu: {PRECOMPUTED_RECS_JSON_FILE}\")\n",
    "    if not matrix_exists: print(f\" - Thiếu: {COSINE_SIM_MATRIX_FILE}\")\n",
    "    if not indices_file_exists: print(f\" - Thiếu: {INDICES_MAP_FILE}\")\n",
    "\n",
    "print(\"\\nBlock 8: Hoàn tất.\")\n",
    "print(\"\\n--- Chương trình kết thúc ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
